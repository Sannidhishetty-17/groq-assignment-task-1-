# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyhL35gE5BZO5sSFQk3VZfB7IFuGSOPJ
"""

# Install required packages
!pip install --quiet openai requests
print("Installed openai + requests")

# Run this cell in Colab and paste your keys when prompted.
from getpass import getpass
import os

# Enter your Groq API key (or your OpenAI-compatible key)
os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ_API_KEY (hidden): ')
# optional: if you have OPENAI_API_KEY instead
# os.environ['OPENAI_API_KEY'] = getpass('Enter OPENAI_API_KEY (optional): ')

print("GROQ_API_KEY set in runtime (won't be saved to notebook file).")

import os, json, time
import openai

def get_groq_client():
    """
    Returns a client-like object. We try to instantiate openai.OpenAI(...) (newer SDK),
    otherwise we fall back to using the openai module (older SDK style).
    """
    api_key = os.environ.get('GROQ_API_KEY') or os.environ.get('OPENAI_API_KEY')
    if not api_key:
        raise RuntimeError("Set GROQ_API_KEY (or OPENAI_API_KEY) in environment (see earlier cell).")

    # preferred: openai.OpenAI instance (newer client)
    try:
        client = openai.OpenAI(api_key=api_key, base_url="https://api.groq.com/openai/v1")
        print("Using openai.OpenAI client object.")
        return client
    except Exception as e:
        # fallback to module-level (older versions)
        openai.api_key = api_key
        openai.api_base = "https://api.groq.com/openai/v1"
        print("Using openai module (fallback).")
        return openai

def call_responses_create(client, **kwargs):
    # Wrapper to call responses.create for different client shapes
    try:
        return client.responses.create(**kwargs)
    except Exception as e1:
        try:
            return openai.Responses.create(**kwargs)
        except Exception as e2:
            raise RuntimeError("responses.create not available. Ensure 'openai' package is up-to-date. Errors: {} / {}".format(e1, e2))

def call_chat_create(client, **kwargs):
    # Wrapper for chat completion function-calling endpoint
    try:
        return client.chat.completions.create(**kwargs)
    except Exception as e1:
        try:
            # older style
            return openai.ChatCompletion.create(**kwargs)
        except Exception as e2:
            raise RuntimeError("chat completion endpoint not available. Ensure 'openai' package is up-to-date. Errors: {} / {}".format(e1, e2))

# Create client now (this will throw if keys missing)
client = get_groq_client()

from typing import List, Dict, Any

class ConversationManager:
    def __init__(self, summarization_k: int = 3, summarization_model: str = "gpt-4o-mini", client=None):
        self.history: List[Dict[str,str]] = []
        self.run_count = 0
        self.summarization_k = summarization_k
        self.summarization_model = summarization_model
        self.client = client or get_groq_client()

    def add_message(self, role: str, content: str):
        self.history.append({"role": role, "content": content})

    def truncate_by_turns(self, n_turns: int) -> List[Dict[str,str]]:
        if n_turns <= 0:
            return list(self.history)
        return self.history[-n_turns:] if n_turns < len(self.history) else list(self.history)

    def truncate_by_chars(self, max_chars: int) -> List[Dict[str,str]]:
        kept = []
        total = 0
        for msg in reversed(self.history):
            l = len(msg["content"])
            if total + l > max_chars:
                break
            kept.append(msg)
            total += l
        return list(reversed(kept))

    def summarize_history(self, prompt_suffix: str = "") -> str:
        # build a short prompt and call the responses API
        combined = "\n\n".join([f"{m['role']}: {m['content']}" for m in self.history])
        prep = "Summarize the following conversation in 2-4 concise bullet points (include intents & actions):\n\n" + combined + "\n\n" + prompt_suffix
        resp = call_responses_create(self.client,
                                    model=self.summarization_model,
                                    input=[{"role":"user","content":prep}],
                                    max_output_tokens=250)
        # defensive extraction of text
        text = ""
        try:
            if hasattr(resp, "output") and isinstance(resp.output, list):
                for item in resp.output:
                    if isinstance(item, dict) and "content" in item:
                        cnt = item["content"]
                        if isinstance(cnt, list):
                            for c in cnt:
                                if isinstance(c, dict) and c.get("type") == "output_text":
                                    text += c.get("text","")
                                elif isinstance(c, str):
                                    text += c
                        elif isinstance(cnt, str):
                            text += cnt
            if not text:
                # fallback: try .to_dict()
                d = resp.to_dict() if hasattr(resp, "to_dict") else dict(resp)
                text = d.get("output", [{}])[0].get("content", [{}])[0].get("text","")
        except Exception as e:
            text = f"(summary error: {e})"
        return (text or "").strip()

    def maybe_summarize_and_replace(self):
        self.run_count += 1
        if self.summarization_k > 0 and (self.run_count % self.summarization_k == 0):
            summary = self.summarize_history()
            self.history = [{"role":"system", "content": f"Summary of prior conversation: {summary}"}]
            return summary
        return None

    def get_history(self):
        return list(self.history)

from openai import OpenAI
import os

# Initialize Groq client
client = OpenAI(
    api_key=os.environ["GROQ_API_KEY"],
    base_url="https://api.groq.com/openai/v1"
)

def chat_completion(messages, model="llama-3.1-8b-instant"):
    """
    Wrapper for Groq chat completion.
    :param messages: List of dict messages [{role: "user"/"assistant"/"system", content: "..."}]
    :param model: Groq model name
    :return: assistant reply (string)
    """
    response = client.chat.completions.create(
        model=model,
        messages=messages
    )
    return response.choices[0].message.content


# Print the assistant's response
print("Assistant:", response.choices[0].message.content)

conversation_history = []

def add_message(role, content):
    """Add a message to the conversation history."""
    conversation_history.append({"role": role, "content": content})

def get_history(limit_turns=None, limit_chars=None):
    """
    Get truncated conversation history.
    - limit_turns: keep only last n messages
    - limit_chars: keep only last X characters
    """
    history = conversation_history.copy()

    if limit_turns:
        history = history[-limit_turns:]

    if limit_chars:
        # Flatten into text, then cut
        flat = "".join([m["content"] for m in history])
        truncated = flat[-limit_chars:]
        history = [{"role": "system", "content": "Truncated conversation:"},
                   {"role": "user", "content": truncated}]
    return history

def summarize_history():
    """Summarize the current conversation history."""
    summary_prompt = [
        {"role": "system", "content": "You are a conversation summarizer."},
        {"role": "user", "content": f"Summarize this chat briefly:\n\n{conversation_history}"}
    ]
    summary = chat_completion(summary_prompt)
    return summary

k = 3  # summarize every 3rd run
run_count = 0

def interact(user_message):
    global run_count, conversation_history
    run_count += 1

    # Add user message
    add_message("user", user_message)

    # Get assistant response
    assistant_reply = chat_completion(conversation_history)
    add_message("assistant", assistant_reply)

    print(f"User: {user_message}")
    print(f"Assistant: {assistant_reply}\n")

    # Every k runs, summarize history
    if run_count % k == 0:
        summary = summarize_history()
        print(f"--- Summary after {run_count} runs ---")
        print(summary)
        # Replace history with summary
        conversation_history = [{"role": "system", "content": "Conversation summary so far:"},
                                {"role": "assistant", "content": summary}]

interact("Hi, can you help me with Python basics?")
interact("What is a function in Python?")
interact("Give me an example of a Python function.")  # after this, summary triggers
interact("Now explain loops in Python.")